{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from h5py import File\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append ('../src')\n",
    "from NeuralNetwork_v2 import SigmoidF,ReluF,TanhF,LReluF,Activation_V2,Linear,Layer,NNClassifier_V2,SoftmaxF, BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from ML_utils import softmax,sigmoid,UTIL_formatY,backward_prop,backpropagation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_data = File(\"data/MNISTdata.hdf5\", 'r')\n",
    "x_train = np.float32(MNIST_data['x_train'][:])\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:, 0])).reshape(-1, 1)\n",
    "x_test  = np.float32(MNIST_data['x_test'][:])\n",
    "y_test  = np.int32(np.array(MNIST_data['y_test'][:, 0])).reshape(-1, 1)\n",
    "MNIST_data.close()\n",
    "\n",
    "# stack together for next step\n",
    "X = np.vstack((x_train, x_test))\n",
    "y = np.vstack((y_train, y_test))\n",
    "\n",
    "# one-hot encoding\n",
    "digits = 10\n",
    "examples = y.shape[0]\n",
    "y = y.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "\n",
    "\n",
    "# number of training set\n",
    "m = 60000\n",
    "m_test = X.shape[0] - m\n",
    "X_train, X_test = X[:m].T, X[m:].T\n",
    "Y_train, Y_test = Y_new[:, :m], Y_new[:, m:]\n",
    "\n",
    "\n",
    "# shuffle training set\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNClassifier Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR = 0.5: 100%|████████████████████████████████████████████████████████████████████████| 25/25 [02:58<00:00,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig set accuracy : 97.91166666666666\n",
      "Test set accuracy : 96.7\n",
      "Classification report for classifier <NeuralNetwork.NNClassifier object at 0x000002C00FD19490>:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.98      0.99      0.98      1135\n",
      "           2       0.98      0.96      0.97      1032\n",
      "           3       0.97      0.96      0.97      1010\n",
      "           4       0.96      0.97      0.97       982\n",
      "           5       0.94      0.96      0.95       892\n",
      "           6       0.98      0.97      0.97       958\n",
      "           7       0.96      0.96      0.96      1028\n",
      "           8       0.97      0.95      0.96       974\n",
      "           9       0.95      0.95      0.95      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from NeuralNetwork import NNClassifier\n",
    "from ML_utils import softmax,sigmoid,UTIL_formatY,backward_prop,backpropagation\n",
    "nn_config={'n_a1':784,'n_a2':128 ,'n_a3':64,'n_a4':25 , 'n_a5':10}   # Configuración de red NN , input layer , hidder layers , output layer\n",
    "sgd_dict =   {'steps':25,'learning_rate':0.5,'mini_batch_size':2**8}\n",
    "opt_dict =   {'maxiter':500,'algorithm' : 'TNC'}\n",
    "activ = {'activation_a2':sigmoid,'activation_a3':sigmoid,'activation_a4':sigmoid,'activation_a5':softmax}\n",
    "method = 'miniBatchGD'\n",
    "if method == 'Optimize':\n",
    "    midict = opt_dict\n",
    "else:\n",
    "    midict = sgd_dict\n",
    "    \n",
    "nn = NNClassifier(optimization=method,bias=True,nn_config=nn_config,activ=activ,debug=False,kargs=midict)\n",
    "\n",
    "costs = nn.optimize (X_train.T,Y_train.T,l2_lambda=0.0)\n",
    "prediction,_ = nn.forward_prop (X_train.T)\n",
    "result = np.argmax(prediction,axis=1).reshape(-1,1)\n",
    "y = np.argmax(Y_train.T,axis=1).reshape(-1,1)\n",
    "\n",
    "accuracy = np.mean(y==result) * 100\n",
    "print ('Trainig set accuracy :' , accuracy  )\n",
    "\n",
    "test_predicted,_ = nn.forward_prop (X_test.T)\n",
    "result = np.argmax(test_predicted,axis=1).reshape(-1,1)\n",
    "y = np.argmax(Y_test.T,axis=1).reshape(-1,1)\n",
    "accuracy = np.mean(y==result) * 100\n",
    "print ('Test set accuracy :' , accuracy  )\n",
    "\n",
    "print(f\"Classification report for classifier {nn}:\\n\"\n",
    "      f\"{classification_report(y,result)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNClassifier_V2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR = 0.5: 100%|████████████████████████████████████████████████████████████████████████| 25/25 [01:57<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig set accuracy : 98.06333333333333\n",
      "Test set accuracy : 96.92\n",
      "Classification report for classifier <NeuralNetwork_v2.NNClassifier_V2 object at 0x000002C06633FB50>:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       980\n",
      "           1       0.97      0.99      0.98      1135\n",
      "           2       0.98      0.97      0.98      1032\n",
      "           3       0.98      0.97      0.97      1010\n",
      "           4       0.97      0.97      0.97       982\n",
      "           5       0.97      0.97      0.97       892\n",
      "           6       0.97      0.97      0.97       958\n",
      "           7       0.97      0.94      0.96      1028\n",
      "           8       0.95      0.97      0.96       974\n",
      "           9       0.95      0.96      0.95      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer (128,784,activationF=SigmoidF,keep_prob=1.0)\n",
    "layer2 = Layer (64,128,activationF=SigmoidF,keep_prob=1.0)\n",
    "layer3 = Layer (25,64,activationF=SigmoidF,keep_prob=1.0)\n",
    "layer4 = Layer (10,25,activationF=SoftmaxF,keep_prob=1.0)\n",
    "layers = [layer1,layer2,layer3,layer4]\n",
    "nn = NNClassifier_V2 (layers,costFunction=CategoricalCrossEntropy,kargs={'steps':25,'learning_rate':0.5,'mini_batch_size':2**8},print_cost=False)\n",
    "costs=nn.optimize (X_train,Y_train,l2_lambda = 0.0,method='miniBatchGD')\n",
    "train_prediction = nn.predict (X_train)\n",
    "result = np.argmax(train_prediction.T,axis=1).reshape(-1,1)\n",
    "y = np.argmax(Y_train.T,axis=1).reshape(-1,1)\n",
    "accuracy = np.mean(y==result) * 100\n",
    "print ('Trainig set accuracy :' , accuracy  )\n",
    "\n",
    "\n",
    "test_predicted = nn.predict (X_test)\n",
    "result = np.argmax(test_predicted.T,axis=1).reshape(-1,1)\n",
    "y = np.argmax(Y_test.T,axis=1).reshape(-1,1)\n",
    "accuracy = np.mean(y==result) * 100\n",
    "print ('Test set accuracy :' , accuracy  )\n",
    "\n",
    "print(f\"Classification report for classifier {nn}:\\n\"\n",
    "      f\"{classification_report(y,result)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNClassifier_V3 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR = 0.5: 100%|████████████████████████████████████████████████████████████████████████| 25/25 [01:09<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig set accuracy : 99.08916666666666\n",
      "Test set accuracy : 98.884\n",
      "Classification report for classifier <NeuralNetwork_v3.NNClassifier_V3 object at 0x00000180BDEE3A30>:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       980\n",
      "           1       0.98      0.98      0.98      1135\n",
      "           2       0.94      0.97      0.95      1032\n",
      "           3       0.98      0.93      0.96      1010\n",
      "           4       0.90      0.95      0.93       982\n",
      "           5       0.93      0.95      0.94       892\n",
      "           6       0.85      0.99      0.91       958\n",
      "           7       0.99      0.92      0.96      1028\n",
      "           8       0.98      0.89      0.93       974\n",
      "           9       0.99      0.84      0.91      1009\n",
      "\n",
      "   micro avg       0.95      0.94      0.94     10000\n",
      "   macro avg       0.95      0.94      0.94     10000\n",
      "weighted avg       0.95      0.94      0.94     10000\n",
      " samples avg       0.94      0.94      0.94     10000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\0012708\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from costs import  BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from NeuralNetwork_v3 import NNClassifier_V3\n",
    "from activationF import SigmoidF,ReluF,TanhF,LReluF,SoftmaxF\n",
    "from sklearn.metrics import multilabel_confusion_matrix,classification_report\n",
    "from GD_algorithms import GradientDescent,Momentum,RMSprop,Adam,LRDecay\n",
    "from layerF import Layer\n",
    "from sklearn.metrics import multilabel_confusion_matrix,classification_report\n",
    "\n",
    "\n",
    "layer1 = Layer (128,784,activationF=SigmoidF,keep_prob=1.0)\n",
    "layer2 = Layer (64,128,activationF=SigmoidF,keep_prob=1.0)\n",
    "layer3 = Layer (25,64,activationF=SigmoidF,keep_prob=1.0)\n",
    "layer4 = Layer (10,25,activationF=SoftmaxF,keep_prob=1.0)\n",
    "layers = [layer1,layer2,layer3,layer4]\n",
    "\n",
    "gd_dict = {'steps':25,   \n",
    "           'learning_rate':0.5,\n",
    "           'decay_rate': 1,\n",
    "           'mini_batch_size':2**8,\n",
    "           'beta':0.9,#used in momentum y RMSprop\n",
    "           'beta1': 0.9 , #used in Adam \n",
    "           'beta2': 0.999 # used in Adam \n",
    "          }\n",
    "\n",
    "nn = NNClassifier_V3 (layers,costFunction=CategoricalCrossEntropy,normalization=False,optimizationF=GradientDescent,kargs=gd_dict,print_cost=False)\n",
    "costs=nn.train (X_train,Y_train,l2_lambda = 0.0,method='miniBatchGD')\n",
    "train_prediction = nn.predict (X_train)\n",
    "train_prediction = train_prediction.T\n",
    "train_prediction [train_prediction > 0.5 ] = 1\n",
    "train_prediction [train_prediction <= 0.5 ] = 0\n",
    "result = train_prediction\n",
    "y = Y_train.T\n",
    "accuracy = np.mean(y==result) * 100\n",
    "print ('Trainig set accuracy :' , accuracy  )\n",
    "\n",
    "\n",
    "test_predicted = nn.predict (X_test)\n",
    "test_predicted = test_predicted.T\n",
    "test_predicted [test_predicted > 0.5 ] = 1\n",
    "test_predicted [test_predicted <= 0.5 ] = 0\n",
    "result = test_predicted\n",
    "y = Y_test.T\n",
    "accuracy = np.mean(y==result) * 100\n",
    "print ('Test set accuracy :' , accuracy  )\n",
    "\n",
    "print(f\"Classification report for classifier {nn}:\\n\"\n",
    "      f\"{classification_report(y,result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
