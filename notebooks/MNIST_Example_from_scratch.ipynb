{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from h5py import File\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append ('../src')\n",
    "from NeuralNetwork import NNClassifier\n",
    "from ML_utils import softmax,sigmoid,UTIL_formatY,backward_prop,backpropagation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix,classification_report\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000, 1)\n"
     ]
    }
   ],
   "source": [
    "MNIST_data = File(\"data/MNISTdata.hdf5\", 'r')\n",
    "x_train = np.float32(MNIST_data['x_train'][:])\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:, 0])).reshape(-1, 1)\n",
    "x_test  = np.float32(MNIST_data['x_test'][:])\n",
    "y_test  = np.int32(np.array(MNIST_data['y_test'][:, 0])).reshape(-1, 1)\n",
    "MNIST_data.close()\n",
    "\n",
    "# stack together for next step\n",
    "X = np.vstack((x_train, x_test))\n",
    "print (X.shape)\n",
    "y = np.vstack((y_train, y_test))\n",
    "print (y.shape)\n",
    "\n",
    "# one-hot encoding\n",
    "digits = 10\n",
    "examples = y.shape[0]\n",
    "y = y.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "\n",
    "\n",
    "# number of training set\n",
    "m = 60000\n",
    "m_test = X.shape[0] - m\n",
    "X_train, X_test = X[:m].T, X[m:].T\n",
    "Y_train, Y_test = Y_new[:, :m], Y_new[:, m:]\n",
    "\n",
    "\n",
    "# shuffle training set\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 784)\n",
      "(64, 1)\n",
      "(10, 64)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "opt = {'n_h':64,'n_x':784,'epochs':10, 'batch_size':64 , 'beta':0.9,'lr':0.5}\n",
    "\n",
    "# initialization\n",
    "params = {\"W1\": np.random.randn(opt['n_h'], opt['n_x']) * np.sqrt(1. / opt['n_x']),\n",
    "          \"b1\": np.zeros((opt['n_h'], 1)) * np.sqrt(1. / opt['n_x']),\n",
    "          \"W2\": np.random.randn(digits, opt['n_h']) * np.sqrt(1. / opt['n_h']),\n",
    "          \"b2\": np.zeros((digits, 1)) * np.sqrt(1. / opt['n_h'])}\n",
    "print (params['W1'].shape)\n",
    "print (params['b1'].shape)\n",
    "print (params['W2'].shape)\n",
    "print (params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 785)\n",
      "(10, 65)\n"
     ]
    }
   ],
   "source": [
    "T1 = np.copy(np.hstack(( params['b1'],params['W1'])))\n",
    "print (T1.shape)\n",
    "T2 = np.copy(np.hstack(( params['b2'],params['W2'],)))\n",
    "print (T2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 785)\n",
      "(10, 65)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn_config={'n_a1':784,'n_a2':64,'n_a3':10 }   # Configuraci√≥n de red NN , input layer , hidder layers , output layer\n",
    "sgd_dict =   {'steps':10,'learning_rate':0.5,'mini_batch_size':2**8}\n",
    "opt_dict =   {'maxiter':100,'algorithm' : 'TNC'}\n",
    "activ = {'activation_a2':sigmoid,'activation_a3':softmax}\n",
    "method = 'miniBatchGD'\n",
    "nn = NNClassifier(optimization=method,bias=True,nn_config=nn_config,activ=activ,debug=False,kargs=sgd_dict)\n",
    "print (nn.thetas['Theta1'].shape)\n",
    "print (nn.thetas['Theta2'].shape)\n",
    "nn.thetas['Theta1'] = T1\n",
    "nn.thetas['Theta2'] = T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid activation function.\n",
    "\n",
    "    inputs: z\n",
    "    outputs: sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s\n",
    "def compute_loss(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    compute loss function\n",
    "    \"\"\"\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L\n",
    "def feed_forward(X, params):\n",
    "    \"\"\"\n",
    "    feed forward network: 2 - layer neural net\n",
    "\n",
    "    inputs:\n",
    "        params: dictionay a dictionary contains all the weights and biases\n",
    "\n",
    "    return:\n",
    "        cache: dictionay a dictionary contains all the fully connected units and activations\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "\n",
    "    # Z1 = W1.dot(x) + b1\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "    \n",
    "    # A1 = sigmoid(Z1)\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "\n",
    "    # Z2 = W2.dot(A1) + b2\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "\n",
    "    # A2 = softmax(Z2)\n",
    "    cache[\"A2\"] = np.exp(cache[\"Z2\"]) / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "#    cache[\"A2\"] = sigmoid(cache[\"Z2\"])\n",
    "\n",
    "    return cache\n",
    "def back_propagate(X, Y, params, cache, m_batch):\n",
    "    \"\"\"\n",
    "    back propagation\n",
    "\n",
    "    inputs:\n",
    "        params: dictionay a dictionary contains all the weights and biases\n",
    "        cache: dictionay a dictionary contains all the fully connected units and activations\n",
    "\n",
    "    return:\n",
    "        grads: dictionay a dictionary contains the gradients of corresponding weights and biases\n",
    "    \"\"\"\n",
    "    # error at last layer\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "\n",
    "    # gradients at last layer (Py2 need 1. to transform to float)\n",
    "    dW2 = (1. / m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1. / m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    # back propgate through first layer\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "\n",
    "    # gradients at first layer (Py2 need 1. to transform to float)\n",
    "    dW1 = (1. / m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1. / m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 1.970743261533071, test loss = 1.9629573743205424\n",
      "Epoch 2: training loss = 1.5994230516340866, test loss = 1.5875651002471802\n",
      "Epoch 3: training loss = 1.2855953514193994, test loss = 1.2678200435568923\n",
      "Epoch 4: training loss = 1.0735109749297258, test loss = 1.0567940953373172\n",
      "Epoch 5: training loss = 0.9041955996535975, test loss = 0.8853720079015203\n",
      "Epoch 6: training loss = 0.8177456112023864, test loss = 0.7986463499783694\n",
      "Epoch 7: training loss = 0.7207538807386803, test loss = 0.7001803659369845\n",
      "Epoch 8: training loss = 0.6841058784790693, test loss = 0.6695612783391528\n",
      "Epoch 9: training loss = 0.6166655116708453, test loss = 0.6008955548506377\n",
      "Epoch 10: training loss = 0.5808964832872017, test loss = 0.5640612607930451\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batches=10\n",
    "for i in range(opt['epochs']):\n",
    "\n",
    "    # shuffle training set\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    X_train_shuffled = X_train[:, permutation]\n",
    "    Y_train_shuffled = Y_train[:, permutation]\n",
    "\n",
    "    for j in range(batches):\n",
    "        #np.testing.assert_allclose (params['W1'],nn.thetas['Theta1'][:,1:])\n",
    "        #np.testing.assert_allclose (params['W2'],nn.thetas['Theta2'][:,1:])\n",
    "        #np.testing.assert_allclose (params['b1'],nn.thetas['Theta1'][:,0:1])\n",
    "        #np.testing.assert_allclose (params['b2'],nn.thetas['Theta2'][:,0:1])\n",
    "\n",
    "        # get mini-batch\n",
    "        begin = j * opt['batch_size']\n",
    "        end = min(begin + opt['batch_size'], X_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        nnX_train = np.copy(X.reshape (X.shape[1],X.shape[0]))\n",
    "        nnY_train = np.copy(Y.reshape (Y.shape[1],Y.shape[0]))\n",
    "\n",
    "        # forward and backward\n",
    "        cache = feed_forward(X, params)\n",
    "        #prediction,nncache = nn.forward_prop (X.T)\n",
    "        #np.testing.assert_allclose (prediction.T,cache['A2'])\n",
    "        grads = back_propagate(X, Y, params, cache, m_batch)\n",
    "        #delta,nngrads = nn.backward_prop (X.T,Y.T)\n",
    "        #np.testing.assert_allclose (grads['dW1'],nngrads['grad1'][:,1:])\n",
    "        #np.testing.assert_allclose (grads['dW2'],nngrads['grad2'][:,1:])\n",
    "        #np.testing.assert_allclose (grads['db1'],nngrads['grad1'][:,0:1])\n",
    "        #np.testing.assert_allclose (grads['db2'],nngrads['grad2'][:,0:1])\n",
    "\n",
    "        # with momentum (optional)\n",
    "        dW1 = grads ['dW1']\n",
    "        db1 = grads ['db1']\n",
    "        dW2 = grads ['dW2']\n",
    "        db2 = grads ['db2']\n",
    "        \n",
    "        dW1 = (opt['beta'] * dW1 + (1. - opt['beta']) * grads[\"dW1\"])\n",
    "        db1 = (opt['beta'] * db1 + (1. - opt['beta']) * grads[\"db1\"])\n",
    "        dW2 = (opt['beta'] * dW2 + (1. - opt['beta']) * grads[\"dW2\"])\n",
    "        db2 = (opt['beta'] * db2 + (1. - opt['beta']) * grads[\"db2\"])\n",
    "    \n",
    "        # gradient descent\n",
    "        params[\"W1\"] = params[\"W1\"] - opt['lr'] * dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - opt['lr'] * db1\n",
    "        params[\"W2\"] = params[\"W2\"] - opt['lr'] * dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - opt['lr'] * db2\n",
    "        #nn._updateThetas (nngrads,opt['lr'])\n",
    "        #np.testing.assert_allclose (params['W1'],nn.thetas['Theta1'][:,1:])\n",
    "        \n",
    "    # forward pass on training set\n",
    "    cache = feed_forward(X_train, params)\n",
    "    #prediction,_ = nn.forward_prop (X_train.T)\n",
    "    train_loss = compute_loss(Y_train, cache[\"A2\"])\n",
    "    #cost = nn.costFunction (X_train.T,Y_train.T)\n",
    "    # forward pass on test set\n",
    "    cache = feed_forward(X_test, params)\n",
    "    test_loss = compute_loss(Y_test, cache[\"A2\"])\n",
    "    print(\"Epoch {}: training loss = {}, test loss = {}\".format(\n",
    "        i + 1, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feed_forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ed0a5abe40a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultilabel_confusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_forward\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feed_forward' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix,classification_report\n",
    "result = feed_forward (X_test,params)\n",
    "prediction = np.argmax(result['A2'],axis=0)\n",
    "print (prediction[0:5])\n",
    "\n",
    "print(f\"Classification report for classifier :\\n\"\n",
    "      f\"{classification_report(y_test,prediction)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR = 0.5:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                   | 3/10 [00:09<00:21,  3.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6beb4787d5fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNNClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnn_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactiv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactiv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml2_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_prop\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tutoriales\\ia\\MachineLearning\\ML\\src\\NeuralNetwork.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, Y, l2_lambda)\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stochasticGD\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml2_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimization\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"miniBatchGD\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_miniBatchGD\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml2_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Valid methods are :'SGD','Optimize','miniBatchGD'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tutoriales\\ia\\MachineLearning\\ML\\src\\NeuralNetwork.py\u001b[0m in \u001b[0;36m_miniBatchGD\u001b[1;34m(self, X, Y, l2_lambda)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0mcosts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_prop\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mminiBatchX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminiBatchY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml2_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_updateThetas\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tutoriales\\ia\\MachineLearning\\ML\\src\\NeuralNetwork.py\u001b[0m in \u001b[0;36mbackward_prop\u001b[1;34m(self, X, Y, l2_lambda)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insertBias\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_prop\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tutoriales\\ia\\MachineLearning\\ML\\src\\NeuralNetwork.py\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mzi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mai\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Theta'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;31m#            ai = sigmoid (zi)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mai\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactiv\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'activation_a'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn_config={'n_a1':784,'n_a2':64,'n_a3':10 }   # Configuraci√≥n de red NN , input layer , hidder layers , output layer\n",
    "sgd_dict =   {'steps':10,'learning_rate':0.5,'mini_batch_size':2**8}\n",
    "opt_dict =   {'maxiter':100,'algorithm' : 'TNC'}\n",
    "activ = {'activation_a2':sigmoid,'activation_a3':softmax}\n",
    "method = 'miniBatchGD'\n",
    "\n",
    "nn = NNClassifier(optimization=method,bias=True,nn_config=nn_config,activ=activ,debug=False,kargs=sgd_dict)\n",
    "costs = nn.optimize (X_train.T,Y_train.T,l2_lambda=0.0)\n",
    "prediction,_ = nn.forward_prop (X_train.T)\n",
    "result = np.argmax(prediction,axis=1).reshape(-1,1)\n",
    "y = np.argmax(Y_train.T,axis=1).reshape(-1,1)\n",
    "\n",
    "accuracy = np.mean(y==result) * 100\n",
    "print ('Trainig set accuracy :' , accuracy  )\n",
    "\n",
    "test_predicted,_ = nn.forward_prop (X_test.T)\n",
    "result = np.argmax(test_predicted,axis=1).reshape(-1,1)\n",
    "y = np.argmax(Y_test.T,axis=1).reshape(-1,1)\n",
    "accuracy = np.mean(y==result) * 100\n",
    "print ('Test set accuracy :' , accuracy  )\n",
    "\n",
    "print(f\"Classification report for classifier {nn}:\\n\"\n",
    "      f\"{classification_report(y,result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR = 0.5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:11<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig set accuracy : 99.58666666666667\n",
      "Test set accuracy : 97.56\n",
      "Classification report for classifier <NeuralNetwork.NNClassifier object at 0x00000257A0091EE0>:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       980\n",
      "           1       0.98      0.99      0.99      1135\n",
      "           2       0.98      0.97      0.97      1032\n",
      "           3       0.98      0.98      0.98      1010\n",
      "           4       0.97      0.98      0.97       982\n",
      "           5       0.98      0.97      0.98       892\n",
      "           6       0.97      0.98      0.98       958\n",
      "           7       0.97      0.97      0.97      1028\n",
      "           8       0.97      0.97      0.97       974\n",
      "           9       0.97      0.96      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_config={'n_a1':784,'n_a2':128 ,'n_a3':64,'n_a4':25 , 'n_a5':10}   # Configuraci√≥n de red NN , input layer , hidder layers , output layer\n",
    "sgd_dict =   {'steps':50,'learning_rate':0.5,'mini_batch_size':2**8}\n",
    "opt_dict =   {'maxiter':500,'algorithm' : 'TNC'}\n",
    "activ = {'activation_a2':sigmoid,'activation_a3':sigmoid,'activation_a4':sigmoid,'activation_a5':softmax}\n",
    "method = 'miniBatchGD'\n",
    "if method == 'Optimize':\n",
    "    midict = opt_dict\n",
    "else:\n",
    "    midict = sgd_dict\n",
    "    \n",
    "nn = NNClassifier(optimization=method,bias=True,nn_config=nn_config,activ=activ,debug=False,kargs=midict)\n",
    "costs = nn.optimize (X_train.T,Y_train.T,l2_lambda=0.0)\n",
    "prediction,_ = nn.forward_prop (X_train.T)\n",
    "result = np.argmax(prediction,axis=1).reshape(-1,1)\n",
    "y = np.argmax(Y_train.T,axis=1).reshape(-1,1)\n",
    "\n",
    "accuracy = np.mean(y==result) * 100\n",
    "print ('Trainig set accuracy :' , accuracy  )\n",
    "\n",
    "test_predicted,_ = nn.forward_prop (X_test.T)\n",
    "result = np.argmax(test_predicted,axis=1).reshape(-1,1)\n",
    "y = np.argmax(Y_test.T,axis=1).reshape(-1,1)\n",
    "accuracy = np.mean(y==result) * 100\n",
    "print ('Test set accuracy :' , accuracy  )H\n",
    "\n",
    "print(f\"Classification report for classifier {nn}:\\n\"\n",
    "      f\"{classification_report(y,result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
