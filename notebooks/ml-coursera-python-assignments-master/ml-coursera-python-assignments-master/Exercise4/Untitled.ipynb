{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the figures buildin the notebook\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import scipy.misc #Used to show matrix as an image\n",
    "from scipy import optimize\n",
    "import matplotlib.cm as cm #Used to display images in a specific colormap\n",
    "import random\n",
    "import sys\n",
    "sys.path.append ('../../../../src/nn') \n",
    "from costs import NN_CostFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"X\" shape:(5000, 401).X[0] shape:(401,)\n",
      "\"y\" shape:(5000, 1). Unique elements in y:[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "fileName = 'data/ex4data1.mat'\n",
    "mat = scipy.io.loadmat(fileName)\n",
    "\n",
    "X = mat['X']\n",
    "Y = mat['y']\n",
    "X = np.insert(X,0,1,axis=1)\n",
    "Y = Y - 1#Y[Y==10]=0   # convert 10 to 0\n",
    "print ('\"X\" shape:%s.X[0] shape:%s' %(X.shape,X[0].shape))\n",
    "print ('\"y\" shape:%s. Unique elements in y:%s' %(Y.shape,np.unique(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of nn_params in reshapeParams is:<class 'numpy.ndarray'>\n",
      "the type of nn_params in nnCostFunction is:<class 'numpy.ndarray'>\n",
      "the type of nn_params in reshapeParams is:<class 'numpy.ndarray'>\n",
      "cost value:0.287629\n",
      "0.2876291651613189\n",
      "the type of nn_params in nnCostFunction is:<class 'numpy.ndarray'>\n",
      "the type of nn_params in reshapeParams is:<class 'numpy.ndarray'>\n",
      "cost value:0.384488\n",
      "0.384487796242894\n"
     ]
    }
   ],
   "source": [
    " def sigmoid(arr, theta):\n",
    "    \"\"\"\n",
    "    function that sigmoid both the input samples and the parameters\n",
    "    \"\"\"\n",
    "    z = np.dot(arr, theta)\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def randInitializeWeights(input_layer_size, hidden_layer_size):\n",
    "    \"\"\"\n",
    "    episilon = np.sqrt(6)/np.sqrt(Lin + Lout)\n",
    "    Lin = the number of input layer unit\n",
    "    Lout = the number of the adjacent layer unit\n",
    "    \"\"\"\n",
    "    episilon = 0.12\n",
    "    return np.random.rand(input_layer_size,hidden_layer_size+1) * 2.0 * episilon - episilon\n",
    "\n",
    "\n",
    "def sigmoidGradient(arr, theta):\n",
    "    sig = sigmoid(arr, theta)\n",
    "    return sig * ( 1 - sig)\n",
    "\n",
    "def reshapeParams(nn_params, input_layer_size=400, hidden_layer_size=25, num_labels=10):\n",
    "    \"\"\"\n",
    "    function is used to reshape the input parameter:theta with type:list as 2 arrays, return it\n",
    "    \"\"\"\n",
    "    print (\"the type of nn_params in reshapeParams is:%s\" % type(nn_params))\n",
    "    theta1 = np.array(nn_params[:(input_layer_size+1) * hidden_layer_size]).reshape((hidden_layer_size,input_layer_size + 1))\n",
    "    theta2 = np.array(nn_params[-num_labels * (hidden_layer_size+1):]).reshape((num_labels, hidden_layer_size+1))\n",
    "    return (theta1, theta2)\n",
    "\n",
    "def formatY(Y,num_labels=10):\n",
    "    result = np.zeros((Y.shape[0],num_labels))\n",
    "    for idx in range(Y.shape[0]):\n",
    "        result[idx,Y[idx,0]] = 1\n",
    "    return result\n",
    "\n",
    "def nnForward (nn_params,  X):\n",
    "    theta1, theta2 = reshapeParams(nn_params, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "    a1 = sigmoid(X, theta1.T) # m * hidden_layer_size\n",
    "    a1 = np.insert(a1,0, 1, axis=1) # m * (hidden_layer_size + 1)\n",
    "    #print a1[:10]\n",
    "    #print \"a1's shape:(%d,%d)\" % a1.shape\n",
    "\n",
    "    a2 = sigmoid(a1, theta2.T) # m * num_labels\n",
    "    return a2\n",
    "\n",
    "def nnCostFunction(nn_params,  X, Y, lamda=0.0,input_layer_size=400, hidden_layer_size=25,\n",
    "                   num_labels=10):\n",
    "    \"\"\"\n",
    "    function to calculate the loss error of the samples\n",
    "    \"\"\"\n",
    "    print (\"the type of nn_params in nnCostFunction is:%s\" % type(nn_params))\n",
    "\n",
    "    theta1, theta2 = reshapeParams(nn_params, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "    a1 = sigmoid(X, theta1.T) # m * hidden_layer_size\n",
    "    a1 = np.insert(a1,0, 1, axis=1) # m * (hidden_layer_size + 1)\n",
    "    #print a1[:10]\n",
    "    #print \"a1's shape:(%d,%d)\" % a1.shape\n",
    "\n",
    "    a2 = sigmoid(a1, theta2.T) # m * num_labels\n",
    "    #print (a2[:10])\n",
    "    #print \"a2's shape:(%d,%d)\" % a2.shape\n",
    "\n",
    "    # format Y from m * 1 to a m*num_labels array\n",
    "    fY = formatY(Y,num_labels)\n",
    "    #print \"Y's shape:(%d,%d)\" % fY.shape\n",
    "    #print (fY)\n",
    "    J = -(np.sum(np.log(a2[fY==1])) + np.sum(np.log(1.0 - a2[fY==0])))\n",
    "    m = len(X)\n",
    "\n",
    "    J = J/m + lamda * (np.sum(theta1**2) + np.sum(theta2**2)) /(2*m)\n",
    "    print (\"cost value:%f\" % J)\n",
    "    return J\n",
    "\n",
    "paramFile = 'data/ex4weights.mat'\n",
    "params = scipy.io.loadmat(paramFile)\n",
    "Theta1 = params['Theta1']\n",
    "Theta2 = params['Theta2']\n",
    "\n",
    "input_layer_size=400 # NO of features of samples\n",
    "hidden_layer_size=25 # NO of Hidden Units \n",
    "num_labels = 10 # NO of Output Units\n",
    "\n",
    "theta = np.append(Theta1.flatten(),Theta2.flatten())\n",
    "\n",
    "test_prediction = nnForward (theta,X)\n",
    "print ( nnCostFunction(theta,X,Y,0.0))\n",
    "\n",
    "print (nnCostFunction(theta,X,Y,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costFunction, regularization value 0.09685863108157507 , with lambda 1.0\n",
      "0.384487796242894\n",
      "(5000, 401)\n",
      "(5000, 25)\n",
      "(5000, 26)\n",
      "(5000, 10)\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "Thetas = [Theta1,Theta2]\n",
    "def sigmoidACV (Z):\n",
    "    return 1 / (1 + np.exp (-Z))\n",
    "\n",
    "def forward_prop (X,Theta):\n",
    "    ai = X \n",
    "    cache = {'a1':X}\n",
    "    n_layers = len (Theta) + 1\n",
    "    for i in range(2,n_layers + 1):\n",
    "        zi = np.dot (ai , Theta [i-2].T )\n",
    "        ai = sigmoidACV (zi)\n",
    "        cache ['z'+str(i)] = zi\n",
    "        if i != n_layers :\n",
    "            ai = np.insert (ai,0,1, axis=1)\n",
    "        cache ['a'+str(i)] = ai\n",
    "\n",
    "    return ai , cache\n",
    "\n",
    "def NNcostFunction ( A_last, Y , Thetas, l2_lambda=0.0 ):\n",
    "    cost = NN_CostFunction (A_last, Y )\n",
    "    if l2_lambda != 0.0:\n",
    "        # Calculamos regularization\n",
    "        m = Y.shape [0]  # Número de muestras\n",
    "        l2_cost = (l2_lambda / ( 2 * m ) ) * reduce (lambda ws, w: ws + np.sum(np.square(w)),Thetas,0)\n",
    "        cost += l2_cost\n",
    "        print ('costFunction, regularization value {} , with lambda {}'.format(l2_cost,l2_lambda))\n",
    "    return cost\n",
    "\n",
    "#ai_t = nnForward (theta,X)\n",
    "ai,cache = forward_prop (X,Thetas)\n",
    "fY = formatY(Y,num_labels)\n",
    "cost = NNcostFunction ( ai , fY, Thetas,l2_lambda = 1.0)\n",
    "print (cost)\n",
    "print (cache['a1'].shape)\n",
    "print (cache['z2'].shape)\n",
    "print (cache['a2'].shape)\n",
    "print (cache['a3'].shape)\n",
    "print (cache['z3'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop (X,Y,Thetas,l2_lambda=0.0):\n",
    "    n_layers = len (Thetas) + 1\n",
    "    m = X.shape [0]\n",
    "    prediction,cache = forward_prop (X,Thetas)\n",
    "    prediction = np.array (prediction)\n",
    "    print ('Prediction shape',prediction.shape)\n",
    "    print ('Y shape' , Y.shape)\n",
    "    delta = {}\n",
    "    deltai = prediction - Y\n",
    "    delta ['delta'+str(n_layers) ] = deltai \n",
    "    for i in reversed (range(2,n_layers)):\n",
    "        print ('Calculating delta layer ', i)\n",
    "        ai = cache['a'+str(i)]\n",
    "        ai = np.delete (ai,0,axis=1)  # Elimino los 1's que no se tienen en cuenta en el cálculo de deltas\n",
    "        gprima_z = ai * ( 1 - ai )\n",
    "        print ('gprima_z shape',gprima_z.shape)\n",
    "        print ('deltai shape',deltai.shape)\n",
    "\n",
    "        step1 = np.dot(deltai,Thetas[i-1][:,1:])\n",
    "        deltaiprev = step1 * gprima_z\n",
    "        delta ['delta'+str(i) ] = deltaiprev \n",
    "        print ('deltaiPREV shape',deltaiprev.shape)\n",
    "        deltai = deltaiprev\n",
    "        \n",
    "    grads = {}\n",
    "    for i in reversed(range (1, n_layers ) ):\n",
    "        print ('Calculating grad2 ' , i)\n",
    "        grad = np.dot (delta['delta'+str(i+1)].T , cache['a'+str(i)] ) / m\n",
    "        grad [:,1:] = grad [:,1:] + (l2_lambda * Thetas[i-1][:,1:] / m )\n",
    "        grads ['grad'+str(i)] = grad\n",
    "        print ('Grad {} shapes delta{} {} a{} {} , grad shape {} ' .format( i,(i+1),delta['delta'+str(i+1)].shape,i, cache['a'+str(i)].shape,grad.shape))\n",
    "        \n",
    "    return delta , grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of nn_params in reshapeParams is:<class 'numpy.ndarray'>\n",
      "Sigmoid gradient - shapes X (5000, 401) theta (25, 401) sigrad (5000, 25)\n",
      "Grad2 shapes delta3 (5000, 10) a2 (5000, 26), grad2 (10, 26) \n",
      "Prediction shape (5000, 10)\n",
      "Y shape (5000, 10)\n",
      "Calculating delta layer  2\n",
      "gprima_z shape (5000, 25)\n",
      "deltai shape (5000, 10)\n",
      "deltaiPREV shape (5000, 25)\n",
      "Calculating grad2  2\n",
      "Grad 2 shapes delta3 (5000, 10) a2 (5000, 26) , grad shape (10, 26) \n",
      "Calculating grad2  1\n",
      "Grad 1 shapes delta2 (5000, 25) a1 (5000, 401) , grad shape (25, 401) \n"
     ]
    }
   ],
   "source": [
    "def backpropagation(nn_params,  X, Y, lamda=0.0,input_layer_size=400, hidden_layer_size=25,\n",
    "                       num_labels=10):\n",
    "    theta1, theta2 = reshapeParams(nn_params, input_layer_size, hidden_layer_size, num_labels)\n",
    "    a2 = sigmoid(X, theta1.T) # m * hidden_layer_size\n",
    "    a2 = np.insert(a2,0, 1, axis=1) # m * (hidden_layer_size + 1)\n",
    "    a3 = sigmoid(a2, theta2.T) # m * num_labels\n",
    "\n",
    "    # format Y from m * 1 to a m*num_labels array\n",
    "    fY = formatY(Y,num_labels)\n",
    "\n",
    "    delta3 = a3 - fY   # m * num_labels\n",
    "    delta2 = np.dot(delta3, theta2[:,1:]) * sigmoidGradient(X, theta1.T)   # m * (hidden_layer_size)\n",
    "    siggrad = sigmoidGradient(X, theta1.T) \n",
    "    print ('Sigmoid gradient - shapes X {} theta {} sigrad {}'.format(X.shape,theta1.shape,siggrad.shape))\n",
    "    \n",
    "    grad2 = np.dot(delta3.T, a2) / X.shape[0] # num_labels * (hidden_layer_size+1)\n",
    "    print ('Grad2 shapes delta3 {} a2 {}, grad2 {} ' .format( delta3.shape,a2.shape,grad2.shape))\n",
    "    grad2[:,1:] = grad2[:,1:] + (lamda * theta2[:,1:]/X.shape[0]) \n",
    "    \n",
    "    grad1 = np.dot(delta2.T, X) / X.shape[0] # (hidden_layer_size) * (input_layer_size+1)\n",
    "    grad1[:,1:] = grad1[:,1:] + (lamda * theta1[:,1:]/X.shape[0])\n",
    "\n",
    "    return np.append(grad1.flatten(),grad2.flatten()) , {'delta3':delta3, 'delta2':delta2 ,'grad2':grad2,'grad1':grad1,'gradienteinicial':siggrad}\n",
    "\n",
    "def computeNumericalGradient(mytheta, X, Y, mylambda=0.0,input_layer_size=400, hidden_layer_size=25,\n",
    "                   num_labels=10):\n",
    "    \"\"\"\n",
    "    mytheta is a flatten array\n",
    "    \"\"\"\n",
    "    print (input_layer_size,hidden_layer_size,num_labels)\n",
    "    print (mytheta.shape)\n",
    "    ngrad = np.zeros((len(mytheta),1))\n",
    "    episode = 0.0001\n",
    "    for i in range(len(mytheta)):\n",
    "        theta_plus = mytheta.copy()\n",
    "        theta_plus[i]=theta_plus[i] + episode\n",
    "        theta_minus = mytheta.copy()\n",
    "        theta_minus[i] = theta_minus[i] - episode\n",
    "        ngrad[i]=(nnCostFunction(theta_plus,  X, Y,mylambda,input_layer_size,hidden_layer_size,num_labels) - nnCostFunction(theta_minus, X, Y,mylambda,input_layer_size,hidden_layer_size,num_labels))/ (2 * episode)\n",
    "\n",
    "    return ngrad\n",
    "\n",
    "def checkNNGradient(mylambda=0.0):\n",
    "    input_layer_size = 3;\n",
    "    hidden_layer_size = 5;\n",
    "    num_labels = 3;\n",
    "    m = 5;\n",
    "    theta1 = randInitializeWeights(hidden_layer_size,input_layer_size);\n",
    "    theta2 = randInitializeWeights(num_labels,hidden_layer_size);\n",
    "    X = randInitializeWeights(m, input_layer_size - 1)\n",
    "    X = np.insert(X,0,1,axis=1)\n",
    "    Y = (np.arange(m) % 3).reshape(m,1)\n",
    "\n",
    "    ngrad = computeNumericalGradient(np.append(theta1.flatten(),theta2.flatten()),X,Y,mylambda,input_layer_size,hidden_layer_size,num_labels)\n",
    "\n",
    "    print (ngrad.shape)\n",
    "\n",
    "    grad = backpropagation(np.append(theta1.flatten(),theta2.flatten()),  X, Y, mylambda,input_layer_size, hidden_layer_size,num_labels)\n",
    "    print (grad.shape)\n",
    "    #print (ngrad.flatten(),grad.flatten())\n",
    "    print (\"%.15f\" % (norm(ngrad.flatten() - grad) / norm(ngrad.flatten() + grad)))\n",
    "\n",
    "def norm(arr):\n",
    "    return np.sqrt(np.dot(arr,arr.T))\n",
    "\n",
    "mylambda = 0.0\n",
    "grad ,deltas= backpropagation(np.append(Theta1.flatten(),Theta2.flatten()),  X, Y, mylambda,input_layer_size, hidden_layer_size,num_labels)\n",
    "mideltas , grads = backward_prop (X,fY,Thetas,l2_lambda=mylambda)\n",
    "assert np.array_equal (deltas['delta3'],mideltas['delta3'])\n",
    "assert np.array_equal (deltas['delta2'],mideltas['delta2'])\n",
    "assert np.array_equal (deltas['grad2'],grads['grad2'])\n",
    "assert np.array_equal (deltas['grad1'],grads['grad1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append ('../../../../src/nn') \n",
    "from layers import FullyConnected\n",
    "from ActivationFunctions import Sigmoid\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "from costs import NN_CostFunction , NN_Gradient\n",
    "\n",
    "def formatY(Y,num_labels=10):\n",
    "    result = np.zeros((Y.shape[0],num_labels))\n",
    "    for idx in range(Y.shape[0]):\n",
    "        result[idx,Y[idx,0]] = 1\n",
    "    return result\n",
    "\n",
    "paramFile = 'data/ex4weights.mat'\n",
    "params = scipy.io.loadmat(paramFile)\n",
    "Theta1 = params['Theta1']\n",
    "Theta2 = params['Theta2']\n",
    "print (Theta1.shape)\n",
    "print (Theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC - HiddenLayer1 - init , initialization with input dimension 400 \n",
      "FC - {} - init , initialization forzada \n",
      "FC - OutputLayer - init , initialization with input dimension 25 \n",
      "FC - {} - init , initialization forzada \n",
      "FC - HiddenLayer1 - forward , starting forward prop\n",
      "FC - HiddenLayer1 - forward , Shapes: A_prev (5000, 400), Z (5000, 25) , A (5000, 25) \n",
      "FC - OutputLayer - forward , starting forward prop\n",
      "FC - OutputLayer - forward , Shapes: A_prev (5000, 25), Z (5000, 10) , A (5000, 10) \n",
      "0.2876291651613189\n",
      "FC - OutputLayer - backward , starting backward prop\n",
      "FC - HiddenLayer1 - backward , starting backward prop\n"
     ]
    }
   ],
   "source": [
    "inp_dimension = 400\n",
    "num_labels=10\n",
    "fY = formatY(Y,num_labels)\n",
    "mX = np.delete (X,0,axis=1)\n",
    "\n",
    "layers = [FullyConnected (25,Sigmoid(),'HiddenLayer1',debug=True,kargs={'W':Theta1[:,1:] , 'b':Theta1[:,0:1]}),\n",
    "          FullyConnected (10,Sigmoid(),'OutputLayer',output_layer=True,debug=True,kargs={'W':Theta2[:,1:] , 'b':Theta2[:,0:1]})\n",
    "         ]\n",
    "nn = NeuralNetwork ( inp_dimension,\n",
    "                    layers, \n",
    "                    NN_CostFunction ,\n",
    "                    NN_Gradient ,\n",
    "                    debug = True,\n",
    "                    l2_lambda = mylambda\n",
    "                    \n",
    "                   )\n",
    "\n",
    "prediction = nn.forward_prop (mX,training=True)\n",
    "#print (prediction.shape)\n",
    "#print (fY.shape)\n",
    "Theta1Layer = np.insert (nn.layers[0].W,0,nn.layers[0].b,axis=1) \n",
    "Theta2Layer = np.insert (nn.layers[1].W,0,nn.layers[1].b,axis=1)\n",
    "assert np.array_equal (Theta1,Theta1Layer)\n",
    "assert np.array_equal (Theta2,Theta2Layer)\n",
    "\n",
    "cost = nn.costFunction (prediction,fY)\n",
    "print (cost)\n",
    "gradsnn = nn.backward_prop(prediction,fY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### grad2 = np.insert (gradsnn['grad2']['W'],0,gradsnn['grad2']['b'],axis=1)\n",
    "grad1 = np.insert (gradsnn['grad1']['W'],0,gradsnn['grad1']['b'],axis=1)\n",
    "r = deltas['grad2'] - grad2\n",
    "np.testing.assert_allclose (deltas['grad2'],grad2,rtol=1e-10, atol=0)\n",
    "np.testing.assert_allclose (deltas['grad1'],grad1,rtol=1e-10, atol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-20\n",
    "def SoftmaxCrossEntropyGrad (a_last,y):\n",
    "    return -np.divide(y,np.clip(a_last,epsilon,1.0))\n",
    "def SigmoidCrossEntropyGrad (a_last,y):\n",
    "    return -np.divide(y,a_last) - np.divide(1-y,1-a_last)\n",
    "\n",
    "ai,cache = forward_prop (X,Thetas)\n",
    "fY = formatY(Y,num_labels)\n",
    "\n",
    "dA = SoftmaxCrossEntropyGrad (ai,fY)\n",
    "a = cache['a3']\n",
    "y = dA * (-a)\n",
    "print (y.shape)\n",
    "dz = a - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append ('../../../../src/nn') \n",
    "from layers import FullyConnected\n",
    "from ActivationFunctions import Sigmoid\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "from costs import NN_CostFunction , NN_Gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "n_layers=3\n",
    "for i in range(2,n_layers+1):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
